{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Install required libraries\n",
        "!pip install torch transformers datasets wandb\n",
        "\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from datasets import load_dataset\n",
        "import logging\n",
        "import wandb\n",
        "from google.colab import files\n",
        "\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "class SurrogateSpikeFunction(torch.autograd.Function):\n",
        "    @staticmethod\n",
        "    def forward(ctx, input):\n",
        "        ctx.save_for_backward(input)\n",
        "        out = (input > 0).float()\n",
        "        return out\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "        input, = ctx.saved_tensors\n",
        "        grad_input = grad_output.clone()\n",
        "        return grad_input\n",
        "\n",
        "surrogate_spike = SurrogateSpikeFunction.apply\n",
        "\n",
        "class AdExNeuron(nn.Module):\n",
        "    def __init__(self, input_size, output_size, tau_m=20.0, tau_w=100.0, a=0.001, b=0.05, V_th=0.0, V_reset=-65.0):\n",
        "        super().__init__()\n",
        "        self.input_size = input_size\n",
        "        self.output_size = output_size\n",
        "        self.tau_m = tau_m\n",
        "        self.tau_w = tau_w\n",
        "        self.a = a\n",
        "        self.b = b\n",
        "        self.V_th = V_th\n",
        "        self.V_reset = V_reset\n",
        "        self.fc = nn.Linear(input_size, output_size)\n",
        "\n",
        "    def forward(self, input_tensor, V, w):\n",
        "        I = self.fc(input_tensor)\n",
        "        dV = (I - w - (V - self.V_reset)) / self.tau_m\n",
        "        dw = (self.a * (V - self.V_reset) - w) / self.tau_w\n",
        "        V += dV\n",
        "        w += dw\n",
        "        mem_pot = V - self.V_th\n",
        "        spikes = surrogate_spike(mem_pot)\n",
        "        V = V * (1 - spikes) + self.V_reset * spikes\n",
        "        w += self.b * spikes\n",
        "        return spikes, V, w\n",
        "\n",
        "class SNNLayer(nn.Module):\n",
        "    def __init__(self, input_size, output_size, num_recurrent_layers=1):\n",
        "        super().__init__()\n",
        "        self.adex = AdExNeuron(input_size, output_size)\n",
        "        self.recurrent_layers = nn.ModuleList([AdExNeuron(output_size, output_size) for _ in range(num_recurrent_layers)])\n",
        "        self.gate = nn.Sigmoid()\n",
        "        self.dropout = nn.Dropout(p=0.2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size = x.size(0)\n",
        "        device = x.device\n",
        "        V = torch.ones(batch_size, self.adex.output_size, device=device) * self.adex.V_reset\n",
        "        w = torch.zeros(batch_size, self.adex.output_size, device=device)\n",
        "        spk_out = []\n",
        "        seq_len = x.size(1)\n",
        "        for t in range(seq_len):\n",
        "            input_t = x[:, t, :]\n",
        "            spk, V, w = self.adex(input_t, V, w)\n",
        "            for layer in self.recurrent_layers:\n",
        "                spk, V, w = layer(spk, V, w)\n",
        "            spk_out.append(spk.unsqueeze(1))\n",
        "        return torch.cat(spk_out, dim=1)\n",
        "\n",
        "class CombinedModel(nn.Module):\n",
        "    def __init__(self, transformer_model, snn_output_size):\n",
        "        super().__init__()\n",
        "        self.transformer = transformer_model\n",
        "        self.snn_layer = SNNLayer(self.transformer.config.hidden_size, snn_output_size)\n",
        "        self.output_layer = nn.Linear(snn_output_size, self.transformer.config.vocab_size)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask=None):\n",
        "        transformer_outputs = self.transformer(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            output_hidden_states=True  # Enable output of hidden states\n",
        "        )\n",
        "        last_hidden_state = transformer_outputs.hidden_states[-1]  # Shape: [batch_size, seq_len, hidden_size]\n",
        "        snn_outputs = self.snn_layer(last_hidden_state)            # Apply SNN layer\n",
        "        logits = self.output_layer(snn_outputs)                    # Shape: [batch_size, seq_len, vocab_size]\n",
        "        return logits\n",
        "\n",
        "def train_model(model, train_dataloader, val_dataloader, num_epochs, learning_rate, device, checkpoint_dir=\"checkpoints\"):\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=0.01)\n",
        "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=2)\n",
        "    if not os.path.exists(checkpoint_dir):\n",
        "        os.makedirs(checkpoint_dir)\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        for batch_idx, batch in enumerate(train_dataloader):\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(input_ids, attention_mask)\n",
        "            shift_logits = outputs[..., :-1, :].contiguous()\n",
        "            shift_labels = input_ids[..., 1:].contiguous()\n",
        "            loss = criterion(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "            if (batch_idx + 1) % 500 == 0:\n",
        "                checkpoint_path = os.path.join(checkpoint_dir, f'model_batch_{epoch+1}_{batch_idx+1}.pth')\n",
        "                torch.save(model.state_dict(), checkpoint_path)\n",
        "                print(f\"Checkpoint saved at {checkpoint_path}\")\n",
        "        avg_train_loss = total_loss / len(train_dataloader)\n",
        "        model.eval()\n",
        "        total_val_loss = 0\n",
        "        with torch.no_grad():\n",
        "            for batch in val_dataloader:\n",
        "                input_ids = batch['input_ids'].to(device)\n",
        "                attention_mask = batch['attention_mask'].to(device)\n",
        "                outputs = model(input_ids, attention_mask)\n",
        "                shift_logits = outputs[..., :-1, :].contiguous()\n",
        "                shift_labels = input_ids[..., 1:].contiguous()\n",
        "                loss = criterion(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
        "                total_val_loss += loss.item()\n",
        "        avg_val_loss = total_val_loss / len(val_dataloader)\n",
        "        scheduler.step(avg_val_loss)\n",
        "        checkpoint_path = os.path.join(checkpoint_dir, f'model_epoch_{epoch+1}.pth')\n",
        "        torch.save(model.state_dict(), checkpoint_path)\n",
        "        print(f\"Checkpoint saved at {checkpoint_path}\")\n",
        "\n",
        "def save_model(model, tokenizer, save_directory):\n",
        "    if not os.path.exists(save_directory):\n",
        "        os.makedirs(save_directory)\n",
        "    model_path = os.path.join(save_directory, 'model_weights.pth')\n",
        "    torch.save(model.state_dict(), model_path)\n",
        "    tokenizer_path = os.path.join(save_directory, 'tokenizer')\n",
        "    tokenizer.save_pretrained(tokenizer_path)\n",
        "\n",
        "def main():\n",
        "    wandb.init(project=\"STAC\")\n",
        "    datasets = load_dataset(\"wikitext\", \"wikitext-2-v1\")\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "    def tokenize_function(examples):\n",
        "        return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=128)\n",
        "    tokenized_datasets = datasets.map(tokenize_function, batched=True, remove_columns=datasets[\"train\"].column_names)\n",
        "    train_dataset = tokenized_datasets['train']\n",
        "    val_dataset = tokenized_datasets['validation']\n",
        "    def collate_fn(batch):\n",
        "        input_ids = torch.tensor([item['input_ids'] for item in batch])\n",
        "        attention_mask = torch.tensor([item['attention_mask'] for item in batch])\n",
        "        return {'input_ids': input_ids, 'attention_mask': attention_mask}\n",
        "    train_dataloader = DataLoader(train_dataset, batch_size=8, shuffle=True, collate_fn=collate_fn)\n",
        "    val_dataloader = DataLoader(val_dataset, batch_size=8, collate_fn=collate_fn)\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    transformer_model = AutoModelForCausalLM.from_pretrained(\"gpt2\").to(device)\n",
        "    combined_model = CombinedModel(transformer_model, snn_output_size=512).to(device)\n",
        "    train_model(combined_model, train_dataloader, val_dataloader, num_epochs=5, learning_rate=5e-5, device=device)\n",
        "    save_directory = \"saved_model\"\n",
        "    save_model(combined_model, tokenizer, save_directory)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "aN-nr7fQexwH"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "machine_shape": "hm",
      "cell_execution_strategy": "setup"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}