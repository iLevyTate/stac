{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_XAzhh8SJ1bm"
      },
      "source": [
        "Install Packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MXzfMfcYDEtK",
        "outputId": "4a815384-4b17-46c5-8eea-e01caa23f4c0"
      },
      "outputs": [],
      "source": [
        "# --- Installation Cell ---\n",
        "# Run this cell first to install required libraries in Google Colab\n",
        "!pip install torch transformers datasets matplotlib torchinfo tqdm accelerate -U -q\n",
        "# accelerate is included as it's often useful with Hugging Face libraries\n",
        "# -U ensures upgrading to latest compatible versions\n",
        "# -q makes the installation quieter\n",
        "print(\"Required libraries installed/updated.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "quTbwZGzJ8YI"
      },
      "source": [
        "Run Tests"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zpJcGBcvDbWY",
        "outputId": "1dd85c61-05b7-40cd-a412-17066fb2aa7a"
      },
      "outputs": [],
      "source": [
        "# --- Test Cell ---\n",
        "# Run this cell before the main script to check component integrity.\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.optim import AdamW  # <--- CORRECTED IMPORT\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from transformers import GPT2Tokenizer, GPT2Model, get_linear_schedule_with_warmup  # <--- AdamW REMOVED\n",
        "import numpy as np\n",
        "import os\n",
        "import tempfile  # For creating temporary directories for checkpoint testing\n",
        "import shutil  # For cleaning up temporary directories\n",
        "import copy  # For comparing states after loading checkpoint\n",
        "import math  # Needed for surrogate spike test\n",
        "import traceback  # For detailed error printing\n",
        "\n",
        "# --- Import necessary components from your main script ---\n",
        "# (These class definitions need to be accessible.)\n",
        "\n",
        "# --- Fallback: Redefine HPARAMS and necessary classes if not in environment ---\n",
        "# This makes the test cell more self-contained if run independently\n",
        "try:\n",
        "    # Check if definitions exist from the main script environment\n",
        "    HPARAMS; SurrogateSpikeFunction; DLPFCAdExNeuron; DLPFCLayer; HyperdimensionalMemoryModule; DLPFCTransformer; save_checkpoint; load_checkpoint; initialize_history\n",
        "    print(\"Using HPARAMS and classes/functions from main script environment.\")\n",
        "except NameError:\n",
        "    print(\"HPARAMS or Classes/Functions not found, defining defaults for testing scope.\")\n",
        "    # --- Minimal HPARAMS for testing ---\n",
        "    HPARAMS = {\n",
        "        'model_name': \"gpt2\",\n",
        "        'learning_rate': 5e-5,\n",
        "        'weight_decay': 0.01,\n",
        "        'l1_lambda': 1e-5,\n",
        "        'num_epochs': 1,\n",
        "        'batch_size': 2,\n",
        "        'seq_length': 16,\n",
        "        'num_recurrent_layers': 1,\n",
        "        'dlpfc_output_size': 8,\n",
        "        'adex_params': {\n",
        "            'tau_m': 20.0,\n",
        "            'tau_w': 144.0,\n",
        "            'a': 4.0,\n",
        "            'b': 0.08,\n",
        "            'V_th': -50.0,\n",
        "            'V_reset': -70.0,\n",
        "            'V_rest': -65.0,\n",
        "            'delta_T': 2.0\n",
        "        },\n",
        "        'dropout_prob': 0.1,\n",
        "        'warmup_steps': 10,\n",
        "        'hdm_dim': 16,\n",
        "        'hdm_hidden_dim': 8,\n",
        "        'log_interval': 10,\n",
        "        'output_dir': os.path.join(tempfile.gettempdir(), \"test_dlpfc_output\"),\n",
        "        'checkpoint_filename': \"checkpoint.pth\",\n",
        "        'best_model_filename': \"best_model_state.pth\",\n",
        "        'final_model_filename': \"final_model_state.pth\",\n",
        "        'history_filename': \"training_history.json\",\n",
        "        'hparams_filename': \"hparams.json\",\n",
        "        'seed': 42\n",
        "    }\n",
        "\n",
        "    # --- Minimal Class Definitions Needed (CORRECT MULTI-LINE __INIT__ SYNTAX) ---\n",
        "\n",
        "    class SurrogateSpikeFunction(torch.autograd.Function):\n",
        "        @staticmethod\n",
        "        def forward(ctx, input_tensor):\n",
        "            ctx.save_for_backward(input_tensor)\n",
        "            return (input_tensor > 0).float()\n",
        "\n",
        "        @staticmethod\n",
        "        def backward(ctx, grad_output):\n",
        "            (input_tensor,) = ctx.saved_tensors\n",
        "            spike_pseudo_grad = torch.exp(-(input_tensor**2) / 2.0) / math.sqrt(2 * math.pi)\n",
        "            return grad_output * spike_pseudo_grad\n",
        "\n",
        "    surrogate_spike = SurrogateSpikeFunction.apply\n",
        "\n",
        "    class DLPFCAdExNeuron(nn.Module):\n",
        "        def __init__(self, **adex_params):\n",
        "            super().__init__()\n",
        "            self.tau_m    = nn.Parameter(torch.tensor(adex_params.get('tau_m', 20.0)))\n",
        "            self.tau_w    = nn.Parameter(torch.tensor(adex_params.get('tau_w', 144.0)))\n",
        "            self.a        = nn.Parameter(torch.tensor(adex_params.get('a', 4.0)))\n",
        "            self.b        = nn.Parameter(torch.tensor(adex_params.get('b', 0.08)))\n",
        "            self.V_th     = nn.Parameter(torch.tensor(adex_params.get('V_th', -50.0)), requires_grad=False)\n",
        "            self.V_reset  = nn.Parameter(torch.tensor(adex_params.get('V_reset', -70.0)), requires_grad=False)\n",
        "            self.V_rest   = nn.Parameter(torch.tensor(adex_params.get('V_rest', -65.0)), requires_grad=False)\n",
        "            self.delta_T  = nn.Parameter(torch.tensor(adex_params.get('delta_T', 2.0)))\n",
        "\n",
        "        def forward(self, input_current, V, w):\n",
        "            dt = 1.0\n",
        "            exp_term = torch.exp((V - self.V_th) / self.delta_T).clamp(max=50.0)\n",
        "            dV = (dt / self.tau_m) * (-(V - self.V_rest) + self.delta_T * exp_term - w + input_current)\n",
        "            V_new = V + dV\n",
        "            dw = (dt / self.tau_w) * (self.a * (V - self.V_rest) - w)\n",
        "            w_new = w + dw\n",
        "            spike = surrogate_spike(V_new - self.V_th)\n",
        "            V_final = torch.where(spike > 0.5, self.V_reset, V_new)\n",
        "            w_final = w_new + self.b * spike\n",
        "            return spike, V_final, w_final\n",
        "\n",
        "    class DLPFCLayer(nn.Module):\n",
        "        def __init__(self, input_size, output_size, num_recurrent_layers=1, adex_params=None, dropout_prob=0.1):\n",
        "            super().__init__()\n",
        "            self.output_size = output_size\n",
        "            self.num_recurrent_layers = num_recurrent_layers\n",
        "            if adex_params is None:\n",
        "                adex_params = {}\n",
        "            self.projection = nn.Linear(input_size, output_size)\n",
        "            self.adex0 = DLPFCAdExNeuron(**adex_params)\n",
        "            self.recurrent_projections = nn.ModuleList([\n",
        "                nn.Linear(output_size, output_size) for _ in range(num_recurrent_layers)\n",
        "            ])\n",
        "            self.recurrent_neurons = nn.ModuleList([\n",
        "                DLPFCAdExNeuron(**adex_params) for _ in range(num_recurrent_layers)\n",
        "            ])\n",
        "            self.dropout = nn.Dropout(p=dropout_prob)\n",
        "\n",
        "        def forward(self, hidden_states):\n",
        "            batch_size, seq_len, _ = hidden_states.size()\n",
        "            device = hidden_states.device\n",
        "            V0 = torch.full((batch_size, self.output_size), self.adex0.V_reset.item(), device=device)\n",
        "            w0 = torch.zeros(batch_size, self.output_size, device=device)\n",
        "            V_rec = [torch.full((batch_size, self.output_size), l.V_reset.item(), device=device) for l in self.recurrent_neurons]\n",
        "            w_rec = [torch.zeros(batch_size, self.output_size, device=device) for _ in self.recurrent_neurons]\n",
        "            spk_list = []\n",
        "            for t in range(seq_len):\n",
        "                x_t = hidden_states[:, t, :]\n",
        "                current = self.projection(x_t)\n",
        "                spk0, V0, w0 = self.adex0(current, V0, w0)\n",
        "                spk_out = self.dropout(spk0)\n",
        "                spk_rec_input = spk_out\n",
        "                for i in range(self.num_recurrent_layers):\n",
        "                    rec_current = self.recurrent_projections[i](spk_rec_input)\n",
        "                    spk_rec, V_rec[i], w_rec[i] = self.recurrent_neurons[i](rec_current, V_rec[i], w_rec[i])\n",
        "                    spk_rec_input = self.dropout(spk_rec)\n",
        "                spk_list.append(spk_rec_input.unsqueeze(1))\n",
        "            return torch.cat(spk_list, dim=1)\n",
        "\n",
        "    class HyperdimensionalMemoryModule(nn.Module):\n",
        "        def __init__(self, input_dim, hdm_dim, output_dim):\n",
        "            super().__init__()\n",
        "            self.register_buffer(\"proj_matrix\", torch.randn(input_dim, hdm_dim))\n",
        "            self.mlp = nn.Sequential(\n",
        "                nn.Linear(hdm_dim, hdm_dim // 2),\n",
        "                nn.ReLU(),\n",
        "                nn.Linear(hdm_dim // 2, output_dim)\n",
        "            )\n",
        "\n",
        "        def forward(self, spike_train):\n",
        "            pooled_spikes = torch.mean(spike_train, dim=1)\n",
        "            hdm_vector = torch.matmul(pooled_spikes, self.proj_matrix)\n",
        "            memory_bias = self.mlp(hdm_vector)\n",
        "            return memory_bias\n",
        "\n",
        "    class DLPFCTransformer(nn.Module):\n",
        "        def __init__(self, hparams):\n",
        "            super().__init__()\n",
        "            self.hparams = hparams\n",
        "            self.gpt2 = GPT2Model.from_pretrained(hparams['model_name'])\n",
        "            gpt2_hidden_size = self.gpt2.config.hidden_size\n",
        "            dlpfc_output_size = hparams['dlpfc_output_size']\n",
        "            self.dlpfc = DLPFCLayer(\n",
        "                gpt2_hidden_size, dlpfc_output_size,\n",
        "                hparams['num_recurrent_layers'], hparams['adex_params'], hparams['dropout_prob']\n",
        "            )\n",
        "            self.memory_module = HyperdimensionalMemoryModule(\n",
        "                dlpfc_output_size, hparams['hdm_dim'], dlpfc_output_size\n",
        "            )\n",
        "            self.dropout = nn.Dropout(p=hparams['dropout_prob'])\n",
        "            self.layer_norm = nn.LayerNorm(dlpfc_output_size)\n",
        "            self.lm_head = nn.Linear(dlpfc_output_size, self.gpt2.config.vocab_size)\n",
        "\n",
        "        def forward(self, input_ids, attention_mask=None):\n",
        "            gpt_out = self.gpt2(input_ids=input_ids, attention_mask=attention_mask)\n",
        "            last_hidden = gpt_out.last_hidden_state\n",
        "            spk_trains = self.dlpfc(last_hidden)\n",
        "            memory_bias = self.memory_module(spk_trains)\n",
        "            memory_bias_unsqueezed = memory_bias.unsqueeze(1)\n",
        "            combined_repr = spk_trains + memory_bias_unsqueezed\n",
        "            combined_repr_norm = self.layer_norm(combined_repr)\n",
        "            combined_repr_drop = self.dropout(combined_repr_norm)\n",
        "            logits = self.lm_head(combined_repr_drop)\n",
        "            return logits, spk_trains\n",
        "\n",
        "    # --- Utility Functions Needed for Checkpoint Test ---\n",
        "    def save_checkpoint(state, filename):\n",
        "        tmp_filename = filename + \".tmp\"\n",
        "        try:\n",
        "            torch.save(state, tmp_filename)\n",
        "            os.rename(tmp_filename, filename)\n",
        "            print(f\"Checkpoint saved to '{filename}' (Epoch {state.get('epoch','N/A')})\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error saving checkpoint: {e}\")\n",
        "            if os.path.exists(tmp_filename):\n",
        "                os.remove(tmp_filename)\n",
        "\n",
        "    def load_checkpoint(checkpoint_path, model, optimizer, scheduler, device):\n",
        "        if os.path.exists(checkpoint_path):\n",
        "            print(f\"Loading checkpoint from '{checkpoint_path}'\")\n",
        "            try:\n",
        "                checkpoint = torch.load(checkpoint_path, map_location='cpu')\n",
        "                model.load_state_dict(checkpoint['model_state_dict'])\n",
        "                model.to(device)\n",
        "                optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "                scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
        "                for state in optimizer.state.values():\n",
        "                    for k, v in state.items():\n",
        "                        if isinstance(v, torch.Tensor):\n",
        "                            state[k] = v.to(device)\n",
        "                start_epoch = checkpoint['epoch'] + 1\n",
        "                best_val_loss = checkpoint.get('best_val_loss', float('inf'))\n",
        "                training_history = checkpoint.get('training_history', initialize_history())\n",
        "                print(f\"Resuming training from epoch {start_epoch}\")\n",
        "                return start_epoch, best_val_loss, training_history\n",
        "            except Exception as e:\n",
        "                import traceback\n",
        "                print(f\"Error loading checkpoint: {e}. Starting fresh.\")\n",
        "                traceback.print_exc()\n",
        "                return 0, float('inf'), initialize_history()\n",
        "        else:\n",
        "            print(\"No checkpoint found. Starting training from scratch.\")\n",
        "            return 0, float('inf'), initialize_history()\n",
        "\n",
        "    def initialize_history():\n",
        "        return {\n",
        "            'epoch': [],\n",
        "            'train_loss': [],\n",
        "            'train_perplexity': [],\n",
        "            'train_l1_loss': [],\n",
        "            'val_loss': [],\n",
        "            'val_perplexity': [],\n",
        "            'val_l1_loss': []\n",
        "        }\n",
        "\n",
        "print(\"--- Setting up Tests ---\")\n",
        "\n",
        "# Use smaller HPARAMS for testing\n",
        "TEST_HPARAMS = copy.deepcopy(HPARAMS)\n",
        "TEST_HPARAMS.update({\n",
        "    'batch_size': 2,\n",
        "    'seq_length': 16,\n",
        "    'dlpfc_output_size': 8,\n",
        "    'hdm_dim': 16,\n",
        "    'num_recurrent_layers': 1,\n",
        "    'num_epochs': 1,\n",
        "    'output_dir': os.path.join(tempfile.gettempdir(), \"test_dlpfc_output\")\n",
        "})\n",
        "\n",
        "# Determine device for testing\n",
        "test_device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Testing on device: {test_device}\")\n",
        "\n",
        "os.makedirs(TEST_HPARAMS['output_dir'], exist_ok=True)\n",
        "\n",
        "# --- Test Functions ---\n",
        "\n",
        "def test_surrogate_spike():\n",
        "    print(\"Testing SurrogateSpikeFunction...\")\n",
        "    input_tensor = torch.randn(5, requires_grad=True, device=test_device) * 0.5  # Leaf tensor\n",
        "    spikes = surrogate_spike(input_tensor)  # Non-leaf tensor\n",
        "    assert spikes.shape == input_tensor.shape, \"Forward shape mismatch\"\n",
        "    assert spikes.dtype == torch.float, \"Forward output dtype mismatch\"\n",
        "    assert torch.all((spikes == 0) | (spikes == 1)), \"Forward output not 0 or 1\"\n",
        "    print(\"  Forward pass OK.\")\n",
        "    dummy_grad = torch.ones_like(spikes)\n",
        "    try:\n",
        "        spikes.backward(dummy_grad)\n",
        "        print(\"  Backward pass executed without error.\")\n",
        "    except Exception as e:\n",
        "        raise AssertionError(f\"Backward pass failed with error: {e}\")\n",
        "    # Check gradient properties on the leaf tensor AFTER backward pass\n",
        "    if input_tensor.grad is not None:\n",
        "        assert input_tensor.grad.shape == input_tensor.shape, f\"Backward grad shape mismatch: {input_tensor.grad.shape}\"\n",
        "        assert input_tensor.grad.dtype == input_tensor.dtype, f\"Backward grad dtype mismatch: {input_tensor.grad.dtype}\"\n",
        "        print(\"  Gradient shape and type on leaf tensor OK.\")\n",
        "    else:\n",
        "        print(\"  Warning: Gradient on leaf tensor is None after backward, but backward executed.\")\n",
        "    print(\"SurrogateSpikeFunction Test PASSED.\")\n",
        "\n",
        "def test_adex_neuron():\n",
        "    print(\"Testing DLPFCAdExNeuron...\")\n",
        "    batch_size = TEST_HPARAMS['batch_size']\n",
        "    output_size = TEST_HPARAMS['dlpfc_output_size']\n",
        "    neuron = DLPFCAdExNeuron(**TEST_HPARAMS['adex_params']).to(test_device)\n",
        "    input_current = torch.randn(batch_size, output_size, device=test_device) * 10\n",
        "    V_init = torch.full((batch_size, output_size), neuron.V_reset.item(), device=test_device)\n",
        "    w_init = torch.zeros(batch_size, output_size, device=test_device)\n",
        "    spike, V_next, w_next = neuron(input_current, V_init, w_init)\n",
        "    assert spike.shape == (batch_size, output_size), f\"Spike shape: {spike.shape}\"\n",
        "    assert V_next.shape == (batch_size, output_size), f\"V_next shape: {V_next.shape}\"\n",
        "    assert w_next.shape == (batch_size, output_size), f\"w_next shape: {w_next.shape}\"\n",
        "    assert spike.dtype == torch.float\n",
        "    assert V_next.dtype == torch.float\n",
        "    assert w_next.dtype == torch.float\n",
        "    print(\"  Output shapes and dtypes OK.\")\n",
        "    params = list(neuron.parameters())\n",
        "    assert len(params) > 0, \"No parameters registered\"\n",
        "    print(f\"  Expected device type: {test_device.type}, index: {test_device.index}\")\n",
        "    all_on_device = True\n",
        "    for name, p in neuron.named_parameters():\n",
        "        param_device = p.device\n",
        "        print(f\"  Param '{name}' device: {param_device}\")\n",
        "        if param_device.type != test_device.type:\n",
        "            all_on_device = False\n",
        "            print(f\"  !!! Type mismatch for '{name}': {param_device.type} != {test_device.type}\")\n",
        "            break\n",
        "        if test_device.type == 'cuda':\n",
        "            expected_index = test_device.index if test_device.index is not None else 0\n",
        "            actual_index = p.device.index if p.device.index is not None else 0\n",
        "            if expected_index != actual_index:\n",
        "                all_on_device = False\n",
        "                print(f\"  !!! Index mismatch for '{name}': {actual_index} != {expected_index}\")\n",
        "                break\n",
        "    assert all_on_device, \"One or more parameters were not moved to the correct device\"\n",
        "    print(\"  Parameters registered and on correct device.\")\n",
        "    print(\"DLPFCAdExNeuron Test PASSED.\")\n",
        "\n",
        "def test_dlpfc_layer():\n",
        "    print(\"Testing DLPFCLayer...\")\n",
        "    batch_size = TEST_HPARAMS['batch_size']\n",
        "    seq_len = TEST_HPARAMS['seq_length']\n",
        "    try:\n",
        "        gpt2_config = GPT2Model.from_pretrained(TEST_HPARAMS['model_name']).config\n",
        "        input_size = gpt2_config.hidden_size\n",
        "    except Exception as e:\n",
        "        print(f\"Warning: Could not load GPT2 config, using default size 768. Error: {e}\")\n",
        "        input_size = 768\n",
        "    output_size = TEST_HPARAMS['dlpfc_output_size']\n",
        "    layer = DLPFCLayer(input_size, output_size, TEST_HPARAMS['num_recurrent_layers'],\n",
        "                        TEST_HPARAMS['adex_params'], TEST_HPARAMS['dropout_prob']).to(test_device)\n",
        "    layer.eval()\n",
        "    hidden_states = torch.randn(batch_size, seq_len, input_size, device=test_device)\n",
        "    with torch.no_grad():\n",
        "        spk_trains = layer(hidden_states)\n",
        "    expected_shape = (batch_size, seq_len, output_size)\n",
        "    assert spk_trains.shape == expected_shape, f\"Output shape mismatch: {spk_trains.shape} vs {expected_shape}\"\n",
        "    assert spk_trains.dtype == torch.float, f\"Output dtype mismatch: {spk_trains.dtype}\"\n",
        "    print(\"  Output shape and dtype OK.\")\n",
        "    print(\"DLPFCLayer Test PASSED.\")\n",
        "\n",
        "def test_memory_module():\n",
        "    print(\"Testing HyperdimensionalMemoryModule...\")\n",
        "    batch_size = TEST_HPARAMS['batch_size']\n",
        "    seq_len = TEST_HPARAMS['seq_length']\n",
        "    input_dim = TEST_HPARAMS['dlpfc_output_size']\n",
        "    hdm_dim = TEST_HPARAMS['hdm_dim']\n",
        "    output_dim = TEST_HPARAMS['dlpfc_output_size']\n",
        "    module = HyperdimensionalMemoryModule(input_dim, hdm_dim, output_dim).to(test_device)\n",
        "    module.eval()\n",
        "    spike_train = torch.randint(0, 2, (batch_size, seq_len, input_dim), dtype=torch.float, device=test_device)\n",
        "    with torch.no_grad():\n",
        "        memory_bias = module(spike_train)\n",
        "    expected_shape = (batch_size, output_dim)\n",
        "    assert memory_bias.shape == expected_shape, f\"Output shape mismatch: {memory_bias.shape} vs {expected_shape}\"\n",
        "    assert memory_bias.dtype == torch.float, f\"Output dtype mismatch: {memory_bias.dtype}\"\n",
        "    print(\"  Output shape and dtype OK.\")\n",
        "    print(\"HyperdimensionalMemoryModule Test PASSED.\")\n",
        "\n",
        "def test_dlpfc_transformer():\n",
        "    print(\"Testing DLPFCTransformer (Full Model Forward Pass)...\")\n",
        "    batch_size = TEST_HPARAMS['batch_size']\n",
        "    seq_len = TEST_HPARAMS['seq_length']\n",
        "    try:\n",
        "        model = DLPFCTransformer(TEST_HPARAMS).to(test_device)\n",
        "        model.eval()\n",
        "        vocab_size = model.gpt2.config.vocab_size\n",
        "    except Exception as e:\n",
        "        raise AssertionError(f\"Failed to instantiate DLPFCTransformer for test: {e}\")\n",
        "    input_ids = torch.randint(0, vocab_size, (batch_size, seq_len), dtype=torch.long, device=test_device)\n",
        "    attention_mask = torch.ones(batch_size, seq_len, dtype=torch.long, device=test_device)\n",
        "    with torch.no_grad():\n",
        "        logits, spk_trains = model(input_ids, attention_mask=attention_mask)\n",
        "    expected_logits_shape = (batch_size, seq_len, vocab_size)\n",
        "    expected_spk_trains_shape = (batch_size, seq_len, TEST_HPARAMS['dlpfc_output_size'])\n",
        "    assert logits.shape == expected_logits_shape, f\"Logits shape mismatch: {logits.shape} vs {expected_logits_shape}\"\n",
        "    assert spk_trains.shape == expected_spk_trains_shape, f\"Spike trains shape mismatch: {spk_trains.shape} vs {expected_spk_trains_shape}\"\n",
        "    assert logits.dtype == torch.float\n",
        "    assert spk_trains.dtype == torch.float\n",
        "    print(\"  Output shapes and dtypes OK.\")\n",
        "    try:\n",
        "        shift_logits = logits[..., :-1, :].contiguous()\n",
        "        shift_labels = input_ids[..., 1:].contiguous()\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "        loss_xent = criterion(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
        "        loss_l1 = TEST_HPARAMS['l1_lambda'] * torch.mean(torch.abs(spk_trains))\n",
        "        total_loss = loss_xent + loss_l1\n",
        "        print(\"  Loss calculation structure compatible with output shapes.\")\n",
        "    except Exception as e:\n",
        "        raise AssertionError(f\"Failed during simulated loss calculation: {e}\")\n",
        "    print(\"DLPFCTransformer Test PASSED.\")\n",
        "\n",
        "def test_data_pipeline():\n",
        "    print(\"Testing Data Pipeline (Tokenization and DataLoader)...\")\n",
        "    dummy_texts = [\"Sentence one.\", \"Sentence two is longer.\", \"Short.\", \"=Title=\"]\n",
        "    dummy_texts_filtered = [text for text in dummy_texts if len(text.strip()) > 0]\n",
        "    class DummyTextDataset(Dataset):\n",
        "        def __init__(self, texts):\n",
        "            self.texts = texts\n",
        "        def __len__(self):\n",
        "            return len(self.texts)\n",
        "        def __getitem__(self, idx):\n",
        "            return {\"text\": self.texts[idx]}\n",
        "    dummy_dataset = DummyTextDataset(dummy_texts_filtered)\n",
        "    try:\n",
        "        tokenizer = GPT2Tokenizer.from_pretrained(TEST_HPARAMS['model_name'])\n",
        "    except Exception as e:\n",
        "        raise AssertionError(f\"Failed to load tokenizer for test: {e}\")\n",
        "    if tokenizer.pad_token is None:\n",
        "        tokenizer.pad_token = tokenizer.eos_token\n",
        "    def tokenize_function_test(examples):\n",
        "        return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=TEST_HPARAMS['seq_length'])\n",
        "    tokenized_data = [tokenize_function_test({\"text\": t}) for t in dummy_dataset.texts]\n",
        "    for item in tokenized_data:\n",
        "        item['input_ids'] = torch.tensor(item['input_ids'], dtype=torch.long)\n",
        "        item['attention_mask'] = torch.tensor(item['attention_mask'], dtype=torch.long)\n",
        "    test_loader = DataLoader(tokenized_data, batch_size=TEST_HPARAMS['batch_size'])\n",
        "    try:\n",
        "        batch = next(iter(test_loader))\n",
        "    except Exception as e:\n",
        "        raise AssertionError(f\"Failed to get batch from DataLoader: {e}\")\n",
        "    assert 'input_ids' in batch and 'attention_mask' in batch\n",
        "    input_ids = batch['input_ids']\n",
        "    attention_mask = batch['attention_mask']\n",
        "    expected_batch_size = min(TEST_HPARAMS['batch_size'], len(tokenized_data))\n",
        "    expected_shape = (expected_batch_size, TEST_HPARAMS['seq_length'])\n",
        "    assert input_ids.shape == expected_shape, f\"Batch input_ids shape: {input_ids.shape} vs {expected_shape}\"\n",
        "    assert attention_mask.shape == expected_shape, f\"Batch attention_mask shape: {attention_mask.shape} vs {expected_shape}\"\n",
        "    assert input_ids.dtype == torch.long and attention_mask.dtype == torch.long\n",
        "    print(\"  Tokenization and DataLoader batch structure OK.\")\n",
        "    print(\"Data Pipeline Test PASSED.\")\n",
        "\n",
        "def test_checkpointing():\n",
        "    print(\"Testing Checkpointing (Save/Load)...\")\n",
        "    test_dir = TEST_HPARAMS['output_dir']\n",
        "    checkpoint_path = os.path.join(test_dir, TEST_HPARAMS['checkpoint_filename'])\n",
        "    try:\n",
        "        model_orig = DLPFCTransformer(TEST_HPARAMS).to(test_device)\n",
        "    except Exception as e:\n",
        "        raise AssertionError(f\"Failed to instantiate model for checkpoint test: {e}\")\n",
        "    optimizer_orig = AdamW(model_orig.parameters(), lr=TEST_HPARAMS['learning_rate'])\n",
        "    scheduler_orig = get_linear_schedule_with_warmup(optimizer_orig, num_warmup_steps=10, num_training_steps=100)\n",
        "    epoch_orig = 3\n",
        "    best_val_loss_orig = 0.123\n",
        "    history_orig = {\n",
        "        'epoch': [1, 2, 3],\n",
        "        'val_loss': [0.5, 0.3, 0.123],\n",
        "        'train_loss': [],\n",
        "        'train_perplexity': [],\n",
        "        'train_l1_loss': [],\n",
        "        'val_perplexity': [],\n",
        "        'val_l1_loss': []\n",
        "    }\n",
        "    optimizer_orig.step()\n",
        "    scheduler_orig.step()\n",
        "    optimizer_orig.step()\n",
        "    scheduler_orig.step()\n",
        "    state_orig = {\n",
        "        'epoch': epoch_orig,\n",
        "        'model_state_dict': model_orig.state_dict(),\n",
        "        'optimizer_state_dict': optimizer_orig.state_dict(),\n",
        "        'scheduler_state_dict': scheduler_orig.state_dict(),\n",
        "        'best_val_loss': best_val_loss_orig,\n",
        "        'training_history': history_orig,\n",
        "        'hparams': TEST_HPARAMS\n",
        "    }\n",
        "    try:\n",
        "        save_checkpoint(state_orig, checkpoint_path)\n",
        "        assert os.path.exists(checkpoint_path), \"Checkpoint file not created\"\n",
        "        print(\"  Save checkpoint OK.\")\n",
        "    except Exception as e:\n",
        "        raise AssertionError(f\"Failed to save checkpoint: {e}\")\n",
        "    model_new = DLPFCTransformer(TEST_HPARAMS).to(test_device)\n",
        "    optimizer_new = AdamW(model_new.parameters(), lr=TEST_HPARAMS['learning_rate'])\n",
        "    scheduler_new = get_linear_schedule_with_warmup(optimizer_new, num_warmup_steps=10, num_training_steps=100)\n",
        "    try:\n",
        "        start_epoch, best_val_loss_loaded, history_loaded = load_checkpoint(checkpoint_path, model_new, optimizer_new, scheduler_new, test_device)\n",
        "        print(\"  Load checkpoint function ran without error.\")\n",
        "    except Exception as e:\n",
        "        raise AssertionError(f\"Failed to load checkpoint: {e}\")\n",
        "    assert start_epoch == epoch_orig + 1, f\"Loaded start_epoch mismatch: {start_epoch} vs {epoch_orig + 1}\"\n",
        "    assert best_val_loss_loaded == best_val_loss_orig, f\"Loaded best_val_loss mismatch: {best_val_loss_loaded} vs {best_val_loss_orig}\"\n",
        "    assert history_loaded == history_orig, \"Loaded training_history mismatch\"\n",
        "    print(\"  Loaded epoch, best_val_loss, history OK.\")\n",
        "    orig_params = list(model_orig.parameters())\n",
        "    new_params = list(model_new.parameters())\n",
        "    assert len(orig_params) == len(new_params) and len(orig_params) > 0, \"Model param list length mismatch or empty model\"\n",
        "    assert torch.equal(orig_params[0], new_params[0]), \"Model state mismatch (first param)\"\n",
        "    assert torch.equal(orig_params[-1], new_params[-1]), \"Model state mismatch (last param)\"\n",
        "    print(\"  Model state loaded OK (checked params).\")\n",
        "    assert len(optimizer_new.param_groups) == len(optimizer_orig.param_groups), \"Optimizer param_groups length mismatch\"\n",
        "    assert scheduler_new.state_dict()['last_epoch'] == scheduler_orig.state_dict()['last_epoch'], \"Scheduler state mismatch (last_epoch)\"\n",
        "    print(\"  Optimizer/Scheduler states loaded OK.\")\n",
        "    print(\"Checkpointing Test PASSED.\")\n",
        "\n",
        "# --- Test Runner ---\n",
        "def run_all_tests():\n",
        "    print(\"\\n--- Running All Tests ---\")\n",
        "    tests_passed = 0\n",
        "    tests_failed = 0\n",
        "    test_functions = [\n",
        "        test_surrogate_spike,\n",
        "        test_adex_neuron,\n",
        "        test_dlpfc_layer,\n",
        "        test_memory_module,\n",
        "        test_dlpfc_transformer,\n",
        "        test_data_pipeline,\n",
        "        test_checkpointing\n",
        "    ]\n",
        "    all_definitions_found = True\n",
        "    try:\n",
        "        HPARAMS\n",
        "        DLPFCAdExNeuron\n",
        "    except NameError:\n",
        "        all_definitions_found = False\n",
        "    if not all_definitions_found:\n",
        "        print(\"\\nWARNING: Running tests using fallback definitions.\\n\")\n",
        "    for test_func in test_functions:\n",
        "        try:\n",
        "            test_func()\n",
        "            tests_passed += 1\n",
        "        except AssertionError as e:\n",
        "            print(f\"!!! Test Failed: {test_func.__name__} !!!\\n  Error: {e}\")\n",
        "            tests_failed += 1\n",
        "        except Exception as e:\n",
        "            import traceback\n",
        "            print(f\"!!! Test Errored: {test_func.__name__} !!!\\n  Unexpected Error: {e}\")\n",
        "            traceback.print_exc()\n",
        "            tests_failed += 1\n",
        "        print(\"-\" * 30)\n",
        "    print(\"\\n--- Test Summary ---\")\n",
        "    print(f\"Tests Passed: {tests_passed}\")\n",
        "    print(f\"Tests Failed: {tests_failed}\")\n",
        "    print(\"--- End of Tests ---\")\n",
        "    # Clean up test directory - uncomment if desired after successful runs\n",
        "    # try:\n",
        "    #     shutil.rmtree(TEST_HPARAMS['output_dir'], ignore_errors=True)\n",
        "    #     print(f\"Cleaned up test directory: {TEST_HPARAMS['output_dir']}\")\n",
        "    # except Exception as e:\n",
        "    #     print(f\"Could not clean up test directory: {e}\")\n",
        "    if tests_failed == 0:\n",
        "        print(\"All tests passed successfully!\")\n",
        "    else:\n",
        "        print(\"Some tests failed. Please review the errors above.\")\n",
        "\n",
        "# --- Execute Tests ---\n",
        "run_all_tests()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JbJHsKyeJ-iP"
      },
      "source": [
        "Run Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "CyQ8iEVnJz0-",
        "outputId": "2837b468-98f3-413a-ab6c-e86cce24a610"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.optim import AdamW\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import GPT2Tokenizer, GPT2Model, get_linear_schedule_with_warmup\n",
        "from datasets import load_dataset\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "import json\n",
        "import math  # For perplexity calculation\n",
        "import time  # For timing epochs\n",
        "from torchinfo import summary  # For model summary\n",
        "import shutil  # For potentially copying best model checkpoint\n",
        "import copy  # For deepcopying HPARAMS\n",
        "import traceback  # For detailed error printing\n",
        "\n",
        "# --------------------------------------------------------------------------------\n",
        "# HYPERPARAMETERS (Defaults) -\n",
        "# --------------------------------------------------------------------------------\n",
        "HPARAMS = {\n",
        "    'model_name': \"gpt2\",         # GPT-2 base\n",
        "    'learning_rate': 5e-5,\n",
        "    'weight_decay': 0.01,\n",
        "    'l1_lambda': 1e-5,            # Penalty on SNN spike activity\n",
        "    'num_epochs': 3,              # Total number of epochs to train for\n",
        "    'batch_size': 8,              # Adjust based on GPU memory (e.g., 4, 8, 16)\n",
        "    'seq_length': 128,            # Max seq length\n",
        "    'num_recurrent_layers': 1,    # Recurrent spiking layers in \"DLPFC\"\n",
        "    'dlpfc_output_size': 512,     # Spiking neurons output dimension\n",
        "    'adex_params': {\n",
        "        'tau_m': 20.0, 'tau_w': 144.0, 'a': 4.0, 'b': 0.08,\n",
        "        'V_th': -50.0, 'V_reset': -70.0, 'V_rest': -65.0, 'delta_T': 2.0\n",
        "    },\n",
        "    'dropout_prob': 0.2,\n",
        "    'warmup_steps': 500,\n",
        "    'hdm_dim': 1024,              # Dimension of the high-dimensional space\n",
        "    'hdm_hidden_dim': 512,        # Not directly used in current simple MLP\n",
        "    'log_interval': 100,          # Log training progress every N steps\n",
        "    'output_dir': \"dlpfc_spiking_gpt2_output\",  # !!! IMPORTANT: Mount Google Drive and point this path there for persistence !!!\n",
        "    'checkpoint_filename': \"checkpoint.pth\",    # Name for the resume checkpoint file\n",
        "    'best_model_filename': \"best_model_state.pth\",  # Name for the best model state file\n",
        "    'final_model_filename': \"final_model_state.pth\",  # Name for the final model state file\n",
        "    'history_filename': \"training_history.json\",  # Name for the training history file\n",
        "    'hparams_filename': \"hparams.json\",           # Name for the hyperparameters file\n",
        "    'seed': 42  # Random seed for reproducibility\n",
        "}  # <--- Closing brace for HPARAMS dictionary\n",
        "\n",
        "# --------------------------------------------------------------------------------\n",
        "# Utility Functions\n",
        "# --------------------------------------------------------------------------------\n",
        "def set_seed(seed_value):\n",
        "    \"\"\"Sets the seed for reproducibility.\"\"\"\n",
        "    np.random.seed(seed_value)\n",
        "    torch.manual_seed(seed_value)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(seed_value)\n",
        "    print(f\"Set random seed to {seed_value}\")\n",
        "\n",
        "def save_checkpoint(state, filename):\n",
        "    \"\"\"Saves checkpoint using atomic write.\"\"\"\n",
        "    tmp_filename = filename + \".tmp\"\n",
        "    try:\n",
        "        torch.save(state, tmp_filename)\n",
        "        os.rename(tmp_filename, filename)  # Atomic rename\n",
        "    except Exception as e:\n",
        "        print(f\"Error saving checkpoint '{filename}': {e}\")\n",
        "        if os.path.exists(tmp_filename):\n",
        "            try:\n",
        "                os.remove(tmp_filename)  # Clean up temp file on error\n",
        "            except OSError:\n",
        "                pass  # Ignore error if removal fails\n",
        "\n",
        "def load_checkpoint(checkpoint_path, model, optimizer, scheduler, device):\n",
        "    \"\"\"Loads checkpoint. Loads to CPU first then moves model to device.\"\"\"\n",
        "    if os.path.exists(checkpoint_path):\n",
        "        print(f\"Loading checkpoint from '{checkpoint_path}'\")\n",
        "        try:\n",
        "            # Load first onto CPU to avoid GPU memory issues\n",
        "            checkpoint = torch.load(checkpoint_path, map_location='cpu')\n",
        "\n",
        "            model.load_state_dict(checkpoint['model_state_dict'])\n",
        "            model.to(device)  # Move model to the correct device *after* loading state_dict\n",
        "\n",
        "            optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "            scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
        "\n",
        "            # Manually move optimizer states to the correct device\n",
        "            for state in optimizer.state.values():\n",
        "                for k, v in state.items():\n",
        "                    if isinstance(v, torch.Tensor):\n",
        "                        state[k] = v.to(device)\n",
        "\n",
        "            start_epoch = checkpoint['epoch'] + 1  # Start from the next epoch\n",
        "            best_val_loss = checkpoint.get('best_val_loss', float('inf'))\n",
        "            training_history = checkpoint.get('training_history', initialize_history())\n",
        "\n",
        "            print(f\"Resuming training from epoch {start_epoch}\")\n",
        "            return start_epoch, best_val_loss, training_history\n",
        "        except FileNotFoundError:\n",
        "            print(f\"Checkpoint file not found at '{checkpoint_path}'. Starting fresh.\")\n",
        "            return 0, float('inf'), initialize_history()\n",
        "        except KeyError as e:\n",
        "            print(f\"Error loading state from checkpoint: Missing key {e}. Checkpoint might be incompatible. Starting fresh.\")\n",
        "            return 0, float('inf'), initialize_history()\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading checkpoint: {e}. Starting fresh.\")\n",
        "            traceback.print_exc()\n",
        "            return 0, float('inf'), initialize_history()\n",
        "    else:\n",
        "        print(f\"No checkpoint found at '{checkpoint_path}'. Starting training from scratch.\")\n",
        "        return 0, float('inf'), initialize_history()\n",
        "\n",
        "def initialize_history():\n",
        "    return {\n",
        "        'epoch': [],\n",
        "        'train_loss': [],\n",
        "        'train_perplexity': [],\n",
        "        'train_l1_loss': [],\n",
        "        'val_loss': [],\n",
        "        'val_perplexity': [],\n",
        "        'val_l1_loss': [],\n",
        "    }\n",
        "\n",
        "# --------------------------------------------------------------------------------\n",
        "# 1) Surrogate Spike Function\n",
        "# --------------------------------------------------------------------------------\n",
        "class SurrogateSpikeFunction(torch.autograd.Function):\n",
        "    @staticmethod\n",
        "    def forward(ctx, input_tensor):\n",
        "        ctx.save_for_backward(input_tensor)\n",
        "        return (input_tensor > 0).float()\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "        (input_tensor,) = ctx.saved_tensors\n",
        "        # Gaussian surrogate gradient\n",
        "        spike_pseudo_grad = torch.exp(-(input_tensor**2) / 2.0) / math.sqrt(2 * math.pi)\n",
        "        return grad_output * spike_pseudo_grad\n",
        "\n",
        "surrogate_spike = SurrogateSpikeFunction.apply\n",
        "\n",
        "# --------------------------------------------------------------------------------\n",
        "# 2) DLPFC AdEx Neuron\n",
        "# --------------------------------------------------------------------------------\n",
        "class DLPFCAdExNeuron(nn.Module):\n",
        "    \"\"\"Minimal AdEx spiking neuron.\"\"\"\n",
        "    def __init__(self, **adex_params):\n",
        "        super().__init__()\n",
        "        self.tau_m = nn.Parameter(torch.tensor(adex_params.get('tau_m', 20.0)))\n",
        "        self.tau_w = nn.Parameter(torch.tensor(adex_params.get('tau_w', 144.0)))\n",
        "        self.a = nn.Parameter(torch.tensor(adex_params.get('a', 4.0)))\n",
        "        self.b = nn.Parameter(torch.tensor(adex_params.get('b', 0.08)))\n",
        "        self.V_th = nn.Parameter(torch.tensor(adex_params.get('V_th', -50.0)), requires_grad=False)\n",
        "        self.V_reset = nn.Parameter(torch.tensor(adex_params.get('V_reset', -70.0)), requires_grad=False)\n",
        "        self.V_rest = nn.Parameter(torch.tensor(adex_params.get('V_rest', -65.0)), requires_grad=False)\n",
        "        self.delta_T = nn.Parameter(torch.tensor(adex_params.get('delta_T', 2.0)))\n",
        "\n",
        "    def forward(self, input_current, V, w):\n",
        "        dt = 1.0  # Assumed time step\n",
        "        exp_term = torch.exp((V - self.V_th) / self.delta_T).clamp(max=50.0)  # Stability clamp\n",
        "        dV = (dt / self.tau_m) * (-(V - self.V_rest) + self.delta_T * exp_term - w + input_current)\n",
        "        V_new = V + dV\n",
        "        dw = (dt / self.tau_w) * (self.a * (V - self.V_rest) - w)\n",
        "        w_new = w + dw\n",
        "        spike = surrogate_spike(V_new - self.V_th)\n",
        "        V_final = torch.where(spike > 0.5, self.V_reset, V_new)\n",
        "        w_final = w_new + self.b * spike\n",
        "        return spike, V_final, w_final\n",
        "\n",
        "# --------------------------------------------------------------------------------\n",
        "# 3) DLPFCLayer\n",
        "# --------------------------------------------------------------------------------\n",
        "class DLPFCLayer(nn.Module):\n",
        "    \"\"\"Processes hidden states sequentially through AdEx neurons.\"\"\"\n",
        "    def __init__(self, input_size, output_size, num_recurrent_layers=1, adex_params=None, dropout_prob=0.1):\n",
        "        super().__init__()\n",
        "        self.output_size = output_size\n",
        "        self.num_recurrent_layers = num_recurrent_layers\n",
        "        if adex_params is None:\n",
        "            adex_params = {}\n",
        "        self.projection = nn.Linear(input_size, output_size)\n",
        "        self.adex0 = DLPFCAdExNeuron(**adex_params)\n",
        "        self.recurrent_projections = nn.ModuleList([\n",
        "            nn.Linear(output_size, output_size) for _ in range(num_recurrent_layers)\n",
        "        ])\n",
        "        self.recurrent_neurons = nn.ModuleList([\n",
        "            DLPFCAdExNeuron(**adex_params) for _ in range(num_recurrent_layers)\n",
        "        ])\n",
        "        self.dropout = nn.Dropout(p=dropout_prob)\n",
        "\n",
        "    def forward(self, hidden_states):\n",
        "        batch_size, seq_len, _ = hidden_states.size()\n",
        "        device = hidden_states.device\n",
        "        # Initialize states\n",
        "        V0 = torch.full((batch_size, self.output_size), self.adex0.V_reset.item(), device=device)\n",
        "        w0 = torch.zeros(batch_size, self.output_size, device=device)\n",
        "        V_rec = [torch.full((batch_size, self.output_size), l.V_reset.item(), device=device) for l in self.recurrent_neurons]\n",
        "        w_rec = [torch.zeros(batch_size, self.output_size, device=device) for _ in self.recurrent_neurons]\n",
        "        spk_list = []\n",
        "        # Iterate through sequence (time steps)\n",
        "        for t in range(seq_len):\n",
        "            x_t = hidden_states[:, t, :]\n",
        "            current = self.projection(x_t)\n",
        "            spk0, V0, w0 = self.adex0(current, V0, w0)\n",
        "            spk_out = self.dropout(spk0)\n",
        "            spk_rec_input = spk_out\n",
        "            # Recurrent layers\n",
        "            for i in range(self.num_recurrent_layers):\n",
        "                rec_current = self.recurrent_projections[i](spk_rec_input)\n",
        "                spk_rec, V_rec[i], w_rec[i] = self.recurrent_neurons[i](rec_current, V_rec[i], w_rec[i])\n",
        "                spk_rec_input = self.dropout(spk_rec)  # Output of last recurrent layer\n",
        "            spk_list.append(spk_rec_input.unsqueeze(1))\n",
        "        return torch.cat(spk_list, dim=1)  # [batch, seq_len, output_size]\n",
        "\n",
        "# --------------------------------------------------------------------------------\n",
        "# 4) Hyperdimensional Memory Module\n",
        "# --------------------------------------------------------------------------------\n",
        "class HyperdimensionalMemoryModule(nn.Module):\n",
        "    \"\"\"Encodes spike train into a single memory bias vector.\"\"\"\n",
        "    def __init__(self, input_dim, hdm_dim, output_dim):\n",
        "        super().__init__()\n",
        "        self.register_buffer(\"proj_matrix\", torch.randn(input_dim, hdm_dim))\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(hdm_dim, hdm_dim // 2),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hdm_dim // 2, output_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, spike_train):\n",
        "        pooled_spikes = torch.mean(spike_train, dim=1)  # [batch, input_dim]\n",
        "        hdm_vector = torch.matmul(pooled_spikes, self.proj_matrix)  # [batch, hdm_dim]\n",
        "        memory_bias = self.mlp(hdm_vector)  # [batch, output_dim]\n",
        "        return memory_bias\n",
        "\n",
        "# --------------------------------------------------------------------------------\n",
        "# 5) DLPFCTransformer\n",
        "# --------------------------------------------------------------------------------\n",
        "class DLPFCTransformer(nn.Module):\n",
        "    \"\"\"Combines GPT-2, DLPFC spiking layer, and HEMM.\"\"\"\n",
        "    def __init__(self, hparams):\n",
        "        super().__init__()\n",
        "        self.hparams = hparams\n",
        "        self.gpt2 = GPT2Model.from_pretrained(hparams['model_name'])\n",
        "        gpt2_hidden_size = self.gpt2.config.hidden_size\n",
        "        dlpfc_output_size = hparams['dlpfc_output_size']\n",
        "        self.dlpfc = DLPFCLayer(\n",
        "            gpt2_hidden_size,\n",
        "            dlpfc_output_size,\n",
        "            hparams['num_recurrent_layers'],\n",
        "            hparams['adex_params'],\n",
        "            hparams['dropout_prob']\n",
        "        )\n",
        "        self.memory_module = HyperdimensionalMemoryModule(\n",
        "            dlpfc_output_size,\n",
        "            hparams['hdm_dim'],\n",
        "            dlpfc_output_size  # Bias dim matches spike dim\n",
        "        )\n",
        "        self.dropout = nn.Dropout(p=hparams['dropout_prob'])\n",
        "        self.layer_norm = nn.LayerNorm(dlpfc_output_size)  # LayerNorm stability\n",
        "        self.lm_head = nn.Linear(dlpfc_output_size, self.gpt2.config.vocab_size)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask=None):\n",
        "        gpt_out = self.gpt2(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        last_hidden = gpt_out.last_hidden_state  # [batch, seq_len, gpt_hidden_size]\n",
        "        spk_trains = self.dlpfc(last_hidden)  # [batch, seq_len, dlpfc_output_size]\n",
        "        memory_bias = self.memory_module(spk_trains)  # [batch, dlpfc_output_size]\n",
        "        # Combine token spikes with memory bias (broadcasted)\n",
        "        memory_bias_unsqueezed = memory_bias.unsqueeze(1)  # [batch, 1, dlpfc_output_size]\n",
        "        combined_repr = spk_trains + memory_bias_unsqueezed  # [batch, seq_len, dlpfc_output_size]\n",
        "        combined_repr_norm = self.layer_norm(combined_repr)\n",
        "        combined_repr_drop = self.dropout(combined_repr_norm)\n",
        "        logits = self.lm_head(combined_repr_drop)  # [batch, seq_len, vocab_size]\n",
        "        return logits, spk_trains\n",
        "\n",
        "# --------------------------------------------------------------------------------\n",
        "# 6) Training Function (Modified for Checkpointing)\n",
        "# --------------------------------------------------------------------------------\n",
        "def train_model(model, train_loader, val_loader, optimizer, scheduler, device, hparams, start_epoch, best_val_loss, training_history):\n",
        "    \"\"\"Trains the model, handling checkpoints and logging.\"\"\"\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    log_interval = hparams['log_interval']\n",
        "    output_dir = hparams['output_dir']\n",
        "    checkpoint_path = os.path.join(output_dir, hparams['checkpoint_filename'])\n",
        "    best_model_path = os.path.join(output_dir, hparams['best_model_filename'])\n",
        "    history_path = os.path.join(output_dir, hparams['history_filename'])\n",
        "    num_epochs = hparams['num_epochs']\n",
        "\n",
        "    print(f\"\\n--- Starting Training (Epochs {start_epoch+1} to {num_epochs}) ---\")\n",
        "    total_start_time = time.time()\n",
        "\n",
        "    if start_epoch >= num_epochs:\n",
        "        print(f\"Start epoch ({start_epoch}) is >= total epochs ({num_epochs}). Training already completed.\")\n",
        "        return training_history  # Return history without further training\n",
        "\n",
        "    for epoch in range(start_epoch, num_epochs):\n",
        "        current_epoch_num = epoch + 1\n",
        "        epoch_start_time = time.time()\n",
        "        model.train()  # Set model to training mode\n",
        "        running_loss, running_l1, steps = 0.0, 0.0, 0\n",
        "        last_log_time = time.time()\n",
        "\n",
        "        batch_iterator = tqdm(train_loader, desc=f\"Epoch {current_epoch_num}/{num_epochs} Training\", leave=False)\n",
        "        for batch in batch_iterator:\n",
        "            # Ensure batch items are tensors and move to device\n",
        "            try:\n",
        "                input_ids = batch['input_ids'].to(device, non_blocking=True)\n",
        "                attention_mask = batch['attention_mask'].to(device, non_blocking=True)\n",
        "            except Exception as e:\n",
        "                print(f\"\\nError processing batch: {e}\")\n",
        "                print(f\"Batch keys: {batch.keys() if isinstance(batch, dict) else 'Not a dict'}\")\n",
        "                if 'input_ids' in batch:\n",
        "                    print(f\"Input IDs type: {type(batch['input_ids'])}\")\n",
        "                if 'attention_mask' in batch:\n",
        "                    print(f\"Attn Mask type: {type(batch['attention_mask'])}\")\n",
        "                continue  # Skip this batch\n",
        "\n",
        "            optimizer.zero_grad(set_to_none=True)  # Use set_to_none for potential memory savings\n",
        "\n",
        "            try:\n",
        "                # Forward pass\n",
        "                logits, spk_trains = model(input_ids, attention_mask=attention_mask)\n",
        "\n",
        "                # Calculate Loss\n",
        "                shift_logits = logits[..., :-1, :].contiguous()\n",
        "                shift_labels = input_ids[..., 1:].contiguous()\n",
        "                loss_xent = criterion(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
        "                loss_l1 = hparams['l1_lambda'] * torch.mean(torch.abs(spk_trains))  # L1 spike penalty\n",
        "                total_loss = loss_xent + loss_l1\n",
        "\n",
        "                # Backward pass and optimization\n",
        "                total_loss.backward()\n",
        "                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)  # Gradient clipping\n",
        "                optimizer.step()\n",
        "                scheduler.step()  # Update learning rate\n",
        "\n",
        "                running_loss += loss_xent.item()\n",
        "                running_l1 += loss_l1.item()\n",
        "                steps += 1\n",
        "            except Exception as e:\n",
        "                print(f\"\\nError during train step: {e}\")\n",
        "                traceback.print_exc()\n",
        "                continue  # Try to continue with next batch\n",
        "\n",
        "            # Log Progress within Epoch\n",
        "            if steps > 0 and (steps % log_interval == 0 or steps == len(train_loader)):\n",
        "                current_time = time.time()\n",
        "                elapsed = current_time - last_log_time\n",
        "                batches_per_sec = log_interval / elapsed if elapsed > 0 else 0\n",
        "                avg_loss = running_loss / steps\n",
        "                avg_l1 = running_l1 / steps\n",
        "                try:\n",
        "                    perplexity = math.exp(avg_loss)\n",
        "                except OverflowError:\n",
        "                    perplexity = float('inf')  # Handle potential overflow\n",
        "                batch_iterator.set_postfix({\n",
        "                    'Step': f'{steps}/{len(train_loader)}',\n",
        "                    'Avg Loss': f'{avg_loss:.4f}',\n",
        "                    'Avg PPL': f'{perplexity:.2f}',\n",
        "                    'Avg L1': f'{avg_l1:.6f}',\n",
        "                    'LR': f'{scheduler.get_last_lr()[0]:.2e}',\n",
        "                    'Batch/s': f'{batches_per_sec:.2f}'\n",
        "                })\n",
        "                last_log_time = time.time()\n",
        "\n",
        "        # --- End of Training Epoch ---\n",
        "        if steps == 0:\n",
        "            print(f\"Epoch {current_epoch_num} had no completed steps. Skipping validation and saving.\")\n",
        "            continue  # Skip to next epoch if no steps were successful\n",
        "\n",
        "        avg_train_loss = running_loss / steps\n",
        "        avg_train_l1 = running_l1 / steps\n",
        "        try:\n",
        "            train_perplexity = math.exp(avg_train_loss)\n",
        "        except OverflowError:\n",
        "            train_perplexity = float('inf')\n",
        "\n",
        "        # --- Validation Phase ---\n",
        "        model.eval()  # Set model to evaluation mode\n",
        "        val_loss, val_l1, val_steps = 0.0, 0.0, 0\n",
        "        val_batch_iterator = tqdm(val_loader, desc=f\"Epoch {current_epoch_num}/{num_epochs} Validation\", leave=False)\n",
        "        with torch.no_grad():\n",
        "            for batch in val_batch_iterator:\n",
        "                try:\n",
        "                    input_ids = batch['input_ids'].to(device, non_blocking=True)\n",
        "                    attention_mask = batch['attention_mask'].to(device, non_blocking=True)\n",
        "                    logits, spk_trains = model(input_ids, attention_mask=attention_mask)\n",
        "                    shift_logits = logits[..., :-1, :].contiguous()\n",
        "                    shift_labels = input_ids[..., 1:].contiguous()\n",
        "                    batch_loss_xent = criterion(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
        "                    batch_loss_l1 = hparams['l1_lambda'] * torch.mean(torch.abs(spk_trains))\n",
        "                    val_loss += batch_loss_xent.item()\n",
        "                    val_l1 += batch_loss_l1.item()\n",
        "                    val_steps += 1\n",
        "                except Exception as e:\n",
        "                    print(f\"\\nError during validation step: {e}\")\n",
        "                    continue\n",
        "\n",
        "        if val_steps == 0:\n",
        "            print(f\"Epoch {current_epoch_num} had no completed validation steps. Using NaN for validation metrics.\")\n",
        "            avg_val_loss, avg_val_l1, val_perplexity = float('nan'), float('nan'), float('nan')\n",
        "        else:\n",
        "            avg_val_loss = val_loss / val_steps\n",
        "            avg_val_l1 = val_l1 / val_steps\n",
        "            try:\n",
        "                val_perplexity = math.exp(avg_val_loss)\n",
        "            except (OverflowError, ValueError):\n",
        "                val_perplexity = float('inf')\n",
        "\n",
        "        epoch_duration = time.time() - epoch_start_time\n",
        "\n",
        "        # --- Log Epoch Results ---\n",
        "        print(f\"\\nEpoch {current_epoch_num}/{num_epochs} completed in {epoch_duration:.2f}s\")\n",
        "        print(f\"  Train Loss: {avg_train_loss:.4f} | Train PPL: {train_perplexity:.2f} | Train L1: {avg_train_l1:.6f}\")\n",
        "        print(f\"  Val Loss:   {avg_val_loss:.4f} | Val PPL:   {val_perplexity:.2f} | Val L1:   {avg_val_l1:.6f}\")\n",
        "\n",
        "        # --- Update Training History ---\n",
        "        safe_train_ppl = train_perplexity if math.isfinite(train_perplexity) else None\n",
        "        safe_val_ppl = val_perplexity if math.isfinite(val_perplexity) else None\n",
        "        safe_avg_val_loss = avg_val_loss if math.isfinite(avg_val_loss) else None\n",
        "        safe_avg_val_l1 = avg_val_l1 if math.isfinite(avg_val_l1) else None\n",
        "\n",
        "        if current_epoch_num not in training_history['epoch']:\n",
        "            training_history['epoch'].append(current_epoch_num)\n",
        "            training_history['train_loss'].append(avg_train_loss)\n",
        "            training_history['train_perplexity'].append(safe_train_ppl)\n",
        "            training_history['train_l1_loss'].append(avg_train_l1)\n",
        "            training_history['val_loss'].append(safe_avg_val_loss)\n",
        "            training_history['val_perplexity'].append(safe_val_ppl)\n",
        "            training_history['val_l1_loss'].append(safe_avg_val_l1)\n",
        "        else:\n",
        "            idx = training_history['epoch'].index(current_epoch_num)\n",
        "            training_history['train_loss'][idx] = avg_train_loss\n",
        "            training_history['train_perplexity'][idx] = safe_train_ppl\n",
        "            training_history['train_l1_loss'][idx] = avg_train_l1\n",
        "            training_history['val_loss'][idx] = safe_avg_val_loss\n",
        "            training_history['val_perplexity'][idx] = safe_val_ppl\n",
        "            training_history['val_l1_loss'][idx] = safe_avg_val_l1\n",
        "            print(f\"  Overwriting history for epoch {current_epoch_num}\")\n",
        "\n",
        "        # --- Checkpoint Saving ---\n",
        "        is_best = False\n",
        "        if math.isfinite(avg_val_loss) and avg_val_loss < best_val_loss:\n",
        "            is_best = True\n",
        "            best_val_loss = avg_val_loss\n",
        "            print(f\"  * New best validation loss found! Saving best model state to '{best_model_path}'\")\n",
        "            try:\n",
        "                torch.save(model.state_dict(), best_model_path)\n",
        "            except Exception as e:\n",
        "                print(f\"  Warning: Failed to save best model state: {e}\")\n",
        "\n",
        "        checkpoint_state = {\n",
        "            'epoch': epoch,  # Save 0-indexed epoch number *completed*\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'scheduler_state_dict': scheduler.state_dict(),\n",
        "            'best_val_loss': best_val_loss,  # Persist the best loss found so far\n",
        "            'training_history': training_history,\n",
        "            'hparams': hparams\n",
        "        }\n",
        "        save_checkpoint(checkpoint_state, checkpoint_path)\n",
        "\n",
        "        try:\n",
        "            serializable_history = copy.deepcopy(training_history)\n",
        "            for key in serializable_history:\n",
        "                serializable_history[key] = [\n",
        "                    x if x is not None and math.isfinite(x) else None for x in serializable_history[key]\n",
        "                ]\n",
        "            with open(history_path, 'w') as f:\n",
        "                json.dump(serializable_history, f, indent=2)\n",
        "        except Exception as e:\n",
        "            print(f\"Warning: Could not save training history JSON: {e}\")\n",
        "\n",
        "    total_duration = time.time() - total_start_time\n",
        "    total_epochs_trained = num_epochs - start_epoch\n",
        "    print(f\"\\n--- Training Finished ({total_epochs_trained} Epochs Trained) ---\")\n",
        "    if total_epochs_trained > 0:\n",
        "        print(f\"Total training time: {total_duration/3600:.2f} hours\")\n",
        "    print(f\"Best validation loss achieved: {best_val_loss:.4f}\")\n",
        "\n",
        "    # --- Plotting ---\n",
        "    valid_epochs = [e for i, e in enumerate(training_history.get('epoch', []))\n",
        "                    if training_history.get('val_loss', [])[i] is not None]\n",
        "    valid_train_loss = [l for i, l in enumerate(training_history.get('train_loss', []))\n",
        "                        if training_history.get('val_loss', [])[i] is not None]\n",
        "    valid_val_loss = [l for l in training_history.get('val_loss', []) if l is not None]\n",
        "    valid_train_ppl = [p for i, p in enumerate(training_history.get('train_perplexity', []))\n",
        "                       if training_history.get('val_loss', [])[i] is not None and p is not None]\n",
        "    valid_val_ppl = [p for p in training_history.get('val_perplexity', []) if p is not None]\n",
        "    valid_train_l1 = [l1 for i, l1 in enumerate(training_history.get('train_l1_loss', []))\n",
        "                      if training_history.get('val_loss', [])[i] is not None]\n",
        "    valid_val_l1 = [l1 for l1 in training_history.get('val_l1_loss', []) if l1 is not None]\n",
        "\n",
        "    if len(valid_epochs) != len(valid_val_loss):\n",
        "        valid_epochs = valid_epochs[:len(valid_val_loss)]\n",
        "    if len(valid_epochs) != len(valid_train_loss):\n",
        "        valid_train_loss = valid_train_loss[:len(valid_epochs)]\n",
        "    if len(valid_epochs) != len(valid_train_ppl):\n",
        "        valid_train_ppl = valid_train_ppl[:len(valid_epochs)]\n",
        "    if len(valid_epochs) != len(valid_val_ppl):\n",
        "        valid_val_ppl = valid_val_ppl[:len(valid_epochs)]\n",
        "    if len(valid_epochs) != len(valid_train_l1):\n",
        "        valid_train_l1 = valid_train_l1[:len(valid_epochs)]\n",
        "    if len(valid_epochs) != len(valid_val_l1):\n",
        "        valid_val_l1 = valid_val_l1[:len(valid_epochs)]\n",
        "\n",
        "    if valid_epochs:\n",
        "        try:\n",
        "            fig, axs = plt.subplots(1, 2, figsize=(16, 5))\n",
        "            axs[0].plot(valid_epochs, valid_train_loss, 'o-', label=\"Train Loss\")\n",
        "            axs[0].plot(valid_epochs, valid_val_loss, 'x-', label=\"Val Loss\")\n",
        "            axs[0].set_xlabel(\"Epoch\")\n",
        "            axs[0].set_ylabel(\"Loss\")\n",
        "            axs[0].set_title(\"Loss\")\n",
        "            axs[0].legend()\n",
        "            axs[0].grid(True)\n",
        "\n",
        "            axs[1].plot(valid_epochs, valid_train_ppl, 'o-', label=\"Train PPL\")\n",
        "            axs[1].plot(valid_epochs, valid_val_ppl, 'x-', label=\"Val PPL\")\n",
        "            axs[1].set_xlabel(\"Epoch\")\n",
        "            axs[1].set_ylabel(\"Perplexity\")\n",
        "            axs[1].set_title(\"Perplexity\")\n",
        "            axs[1].legend()\n",
        "            axs[1].grid(True)\n",
        "            axs[1].set_yscale('log')\n",
        "            plt.tight_layout()\n",
        "            plot_path = os.path.join(output_dir, \"loss_perplexity_curves.png\")\n",
        "            plt.savefig(plot_path)\n",
        "            print(f\"Loss/perplexity plot saved to {plot_path}\")\n",
        "            plt.show()\n",
        "\n",
        "            plt.figure(figsize=(8, 5))\n",
        "            plt.plot(valid_epochs, valid_train_l1, 'o-', label=\"Train L1\")\n",
        "            plt.plot(valid_epochs, valid_val_l1, 'x-', label=\"Val L1\")\n",
        "            plt.xlabel(\"Epoch\")\n",
        "            plt.ylabel(\"L1 Loss\")\n",
        "            plt.title(\"Spike L1 Regularization\")\n",
        "            plt.legend()\n",
        "            plt.grid(True)\n",
        "            plt.tight_layout()\n",
        "            l1_plot_path = os.path.join(output_dir, \"l1_loss_curve.png\")\n",
        "            plt.savefig(l1_plot_path)\n",
        "            print(f\"L1 loss plot saved to {l1_plot_path}\")\n",
        "            plt.show()\n",
        "        except Exception as plot_err:\n",
        "            print(f\"Error generating plots: {plot_err}\")\n",
        "    else:\n",
        "        print(\"No valid training history found to plot.\")\n",
        "\n",
        "    return training_history\n",
        "\n",
        "# --------------------------------------------------------------------------------\n",
        "# 7) Main Execution Block\n",
        "# --------------------------------------------------------------------------------\n",
        "if __name__ == \"__main__\":\n",
        "    # --- Basic Setup ---\n",
        "    set_seed(HPARAMS['seed'])\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"Using device: {device}\")\n",
        "    output_dir = HPARAMS['output_dir']\n",
        "\n",
        "    # --- !!! IMPORTANT FOR COLAB PERSISTENCE !!! ---\n",
        "    # Mount Google Drive before running this cell if using Colab.\n",
        "    try:\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "        print(f\"Output directory confirmed: '{output_dir}'\")\n",
        "    except OSError as e:\n",
        "        print(f\"CRITICAL Error creating output directory '{output_dir}': {e}\")\n",
        "        print(\"Please ensure the path is valid and accessible. Exiting.\")\n",
        "        exit()\n",
        "\n",
        "    # --- Save Hyperparameters ---\n",
        "    hparams_path = os.path.join(output_dir, HPARAMS['hparams_filename'])\n",
        "    try:\n",
        "        with open(hparams_path, \"w\") as f:\n",
        "            json.dump(HPARAMS, f, indent=2)\n",
        "        print(f\"Hyperparameters saved to '{hparams_path}'\")\n",
        "    except Exception as e:\n",
        "        print(f\"Warning: Could not save hyperparameters: {e}\")\n",
        "\n",
        "    # --- Load and Tokenize Data ---\n",
        "    print(\"\\nLoading and preparing dataset...\")\n",
        "    try:\n",
        "        raw_data = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\")\n",
        "        raw_data = raw_data.filter(lambda x: x['text'] and x['text'].strip())\n",
        "        if not raw_data['train']:\n",
        "            print(\"CRITICAL Error: No valid training data found after filtering empty lines. Exiting.\")\n",
        "            exit()\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to load dataset: {e}. Exiting.\")\n",
        "        exit()\n",
        "\n",
        "    print(\"Loading tokenizer...\")\n",
        "    try:\n",
        "        tokenizer = GPT2Tokenizer.from_pretrained(HPARAMS['model_name'])\n",
        "        if tokenizer.pad_token is None:\n",
        "            tokenizer.pad_token = tokenizer.eos_token\n",
        "            print(f\"Set tokenizer pad_token to eos_token ({tokenizer.eos_token})\")\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to load tokenizer '{HPARAMS['model_name']}': {e}. Exiting.\")\n",
        "        exit()\n",
        "\n",
        "    def tokenize_function(examples):\n",
        "        return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=HPARAMS['seq_length'])\n",
        "\n",
        "    print(\"Tokenizing dataset (this might take a while)...\")\n",
        "    try:\n",
        "        num_cpus = os.cpu_count()\n",
        "        num_proc = min(4, num_cpus if num_cpus is not None else 1)\n",
        "        print(f\"Using {num_proc} processes for tokenization.\")\n",
        "        tokenized = raw_data.map(tokenize_function, batched=True, num_proc=num_proc, remove_columns=raw_data[\"train\"].column_names)\n",
        "        train_data = tokenized[\"train\"]\n",
        "        val_data = tokenized[\"validation\"]\n",
        "        train_data.set_format(type='torch', columns=['input_ids', 'attention_mask'])\n",
        "        val_data.set_format(type='torch', columns=['input_ids', 'attention_mask'])\n",
        "        if len(train_data) == 0:\n",
        "            print(\"CRITICAL Error: Training data is empty after tokenization. Exiting.\")\n",
        "            exit()\n",
        "    except Exception as e:\n",
        "        print(f\"Failed during dataset tokenization: {e}. Exiting.\")\n",
        "        traceback.print_exc()\n",
        "        exit()\n",
        "\n",
        "    try:\n",
        "        pin_memory_flag = True if device.type == 'cuda' else False\n",
        "        num_workers = min(2, num_cpus if num_cpus is not None else 1)\n",
        "        print(f\"Using {num_workers} workers for DataLoaders.\")\n",
        "        train_loader = DataLoader(train_data, batch_size=HPARAMS['batch_size'], shuffle=True,\n",
        "                                  num_workers=num_workers, pin_memory=pin_memory_flag, drop_last=True)\n",
        "        val_loader = DataLoader(val_data, batch_size=HPARAMS['batch_size'], shuffle=False,\n",
        "                                num_workers=num_workers, pin_memory=pin_memory_flag)\n",
        "        print(f\"DataLoaders ready: Train batches={len(train_loader)}, Val batches={len(val_loader)}\")\n",
        "        if len(train_loader) == 0:\n",
        "            print(\"CRITICAL Error: Training DataLoader has zero batches. Check batch size and dataset size. Exiting.\")\n",
        "            exit()\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to create DataLoaders: {e}. Exiting.\")\n",
        "        exit()\n",
        "\n",
        "    print(\"\\nInstantiating model, optimizer, and scheduler...\")\n",
        "    try:\n",
        "        model = DLPFCTransformer(HPARAMS).to(device)\n",
        "        optimizer = AdamW(model.parameters(), lr=HPARAMS['learning_rate'], weight_decay=HPARAMS['weight_decay'])\n",
        "        full_total_steps = len(train_loader) * HPARAMS['num_epochs']\n",
        "        scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=HPARAMS['warmup_steps'],\n",
        "                                                    num_training_steps=full_total_steps)\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to instantiate model/optimizer/scheduler: {e}. Exiting.\")\n",
        "        traceback.print_exc()\n",
        "        exit()\n",
        "\n",
        "    print(\"\\n--- Model Architecture ---\")\n",
        "    try:\n",
        "        summary_batch_size = HPARAMS['batch_size']\n",
        "        sample_input_ids = torch.zeros((summary_batch_size, HPARAMS['seq_length']), dtype=torch.long).to(device)\n",
        "        sample_attention_mask = torch.ones((summary_batch_size, HPARAMS['seq_length']), dtype=torch.long).to(device)\n",
        "        summary(model, input_data=(sample_input_ids, sample_attention_mask), depth=5,\n",
        "                col_names=[\"input_size\", \"output_size\", \"num_params\", \"mult_adds\"], row_settings=[\"var_names\"])\n",
        "    except ImportError:\n",
        "        print(\"torchinfo not found. Install (`pip install torchinfo`) for detailed summary.\")\n",
        "        print(model)\n",
        "    except Exception as e:\n",
        "        print(f\"Could not generate model summary: {e}\\n{model}\")\n",
        "    try:\n",
        "        num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "        print(f\"Total Trainable Parameters: {num_params:,}\")\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    checkpoint_path = os.path.join(output_dir, HPARAMS['checkpoint_filename'])\n",
        "    start_epoch, best_val_loss, training_history = load_checkpoint(checkpoint_path, model, optimizer, scheduler, device)\n",
        "    if not isinstance(training_history, dict) or not all(k in training_history for k in initialize_history().keys()):\n",
        "        print(\"Loaded training history is invalid or incomplete. Reinitializing.\")\n",
        "        training_history = initialize_history()\n",
        "\n",
        "    try:\n",
        "        training_history = train_model(\n",
        "            model, train_loader, val_loader, optimizer, scheduler, device,\n",
        "            HPARAMS, start_epoch, best_val_loss, training_history\n",
        "        )\n",
        "    except Exception as train_err:\n",
        "        print(\"\\n--- CRITICAL ERROR DURING TRAINING ---\")\n",
        "        print(f\"{train_err}\")\n",
        "        traceback.print_exc()\n",
        "        print(\"Attempting to save final state before exiting...\")\n",
        "\n",
        "    print(\"\\nSaving final model components...\")\n",
        "    final_model_path = os.path.join(output_dir, HPARAMS['final_model_filename'])\n",
        "    try:\n",
        "        torch.save(model.state_dict(), final_model_path)\n",
        "        print(f\"Final model state_dict saved to '{final_model_path}'\")\n",
        "    except Exception as e:\n",
        "        print(f\"Warning: Could not save final model state: {e}\")\n",
        "\n",
        "    try:\n",
        "        tokenizer.save_pretrained(output_dir)\n",
        "        print(f\"Tokenizer saved to '{output_dir}'\")\n",
        "    except Exception as e:\n",
        "        print(f\"Warning: Could not save tokenizer: {e}\")\n",
        "\n",
        "    history_path = os.path.join(output_dir, HPARAMS['history_filename'])\n",
        "    try:\n",
        "        serializable_history = copy.deepcopy(training_history)\n",
        "        for key in serializable_history:\n",
        "            if isinstance(serializable_history[key], list):\n",
        "                serializable_history[key] = [\n",
        "                    x if x is not None and isinstance(x, (int, float)) and math.isfinite(x) else None\n",
        "                    for x in serializable_history[key]\n",
        "                ]\n",
        "        with open(history_path, 'w') as f:\n",
        "            json.dump(serializable_history, f, indent=2)\n",
        "        print(f\"Final training history saved to '{history_path}'\")\n",
        "    except Exception as e:\n",
        "        print(f\"Warning: Could not save final training history JSON: {e}\")\n",
        "\n",
        "    print(\"\\n--- Script Execution Complete ---\")\n",
        "    print(f\"Find results, checkpoints, and plots in: {output_dir}\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "cell_execution_strategy": "setup",
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
